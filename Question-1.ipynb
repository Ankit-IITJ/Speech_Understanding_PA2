{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c73917-ac60-4baf-bacc-742f0182e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "For II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584dcb59-f1ad-47b1-9086-12795d8c931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with optimizations...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6451/1006186086.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu', mmap=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                              | 0/7458 [00:00<?, ?it/s]/tmp/ipykernel_6451/1006186086.py:274: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
      "Training: 100%|████████████████| 7458/7458 [11:16:38<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 16.3411, Val Loss: 19.7801\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import tempfile\n",
    "from omegaconf import OmegaConf\n",
    "from fairseq.models.wav2vec.wav2vec2 import Wav2Vec2Config, Wav2Vec2Model\n",
    "import gc\n",
    "import psutil\n",
    "import soundfile as sf\n",
    "import io\n",
    "\n",
    "# --------------------------\n",
    "# 1. Configuration for Optimization\n",
    "# --------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_num_threads(os.cpu_count()) if device.type == 'cpu' else None\n",
    "os.environ['OMP_NUM_THREADS'] = str(os.cpu_count())\n",
    "os.environ['MKL_NUM_THREADS'] = str(os.cpu_count())\n",
    "\n",
    "# --------------------------\n",
    "# 2. Memory-Efficient Audio Loading\n",
    "# --------------------------\n",
    "def load_audio(path, sample_rate=16000):\n",
    "    \"\"\"Optimized audio loading that minimizes disk usage\"\"\"\n",
    "    try:\n",
    "        # First try direct loading for supported formats\n",
    "        if path.lower().endswith(('.wav', '.flac')):\n",
    "            audio, _ = sf.read(path, dtype='float32', always_2d=False)\n",
    "        else:\n",
    "            # Use in-memory conversion for unsupported formats\n",
    "            cmd = [\n",
    "                'ffmpeg', '-y', '-i', path,\n",
    "                '-ac', '1', '-ar', str(sample_rate),\n",
    "                '-f', 'wav', '-'\n",
    "            ]\n",
    "            process = subprocess.Popen(\n",
    "                cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
    "                stdin=subprocess.DEVNULL\n",
    "            )\n",
    "            stdout, _ = process.communicate()\n",
    "            \n",
    "            # Read directly from memory\n",
    "            audio, _ = sf.read(io.BytesIO(stdout), dtype='float32')\n",
    "\n",
    "        if len(audio) == 0 or np.max(np.abs(audio)) < 0.001:\n",
    "            return np.zeros(sample_rate * 3, dtype=np.float32)\n",
    "\n",
    "        return audio.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {str(e)}\")\n",
    "        return np.zeros(sample_rate * 3, dtype=np.float32)\n",
    "\n",
    "# --------------------------\n",
    "# 3. Optimized Dataset Class\n",
    "# --------------------------\n",
    "class VoxCelebDataset(Dataset):\n",
    "    def __init__(self, root_dir, ids, sample_rate=16000, duration=3):\n",
    "        self.root_dir = root_dir\n",
    "        self.ids = ids\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_samples = sample_rate * duration\n",
    "        self.speaker_to_idx = {speaker: idx for idx, speaker in enumerate(ids)}\n",
    "        self.samples = []\n",
    "\n",
    "        # Build file list more efficiently\n",
    "        for speaker in ids:\n",
    "            speaker_dir = os.path.join(root_dir, speaker)\n",
    "            if not os.path.exists(speaker_dir):\n",
    "                continue\n",
    "                \n",
    "            for root, _, files in os.walk(speaker_dir):\n",
    "                for file in files:\n",
    "                    if file.endswith(('.wav', '.flac', '.m4a')):\n",
    "                        full_path = os.path.join(root, file)\n",
    "                        self.samples.append((full_path, self.speaker_to_idx[speaker]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        audio = load_audio(path, self.sample_rate)\n",
    "\n",
    "        # More efficient padding/trimming\n",
    "        if len(audio) > self.max_samples:\n",
    "            start = np.random.randint(0, len(audio) - self.max_samples)\n",
    "            audio = audio[start:start+self.max_samples]\n",
    "        elif len(audio) < self.max_samples:\n",
    "            padding = self.max_samples - len(audio)\n",
    "            audio = np.pad(audio, (0, padding), mode='constant')\n",
    "\n",
    "        return torch.from_numpy(audio), label\n",
    "\n",
    "# --------------------------\n",
    "# 4. Model Loading (Optimized)\n",
    "# --------------------------\n",
    "def load_fairseq_model(checkpoint_path):\n",
    "    \"\"\"Load model with memory optimizations\"\"\"\n",
    "    # Load with mmap for large files\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu', mmap=True)\n",
    "    cfg_dict = checkpoint['model_cfg']\n",
    "\n",
    "    if 'final_dim' in cfg_dict and cfg_dict['final_dim'] == 768:\n",
    "        cfg_dict['final_dim'] = 1024\n",
    "\n",
    "    cfg = Wav2Vec2Config(\n",
    "        extractor_mode=cfg_dict.get('extractor_mode', 'default'),\n",
    "        encoder_layers=cfg_dict.get('encoder_layers', 12),\n",
    "        encoder_embed_dim=cfg_dict.get('encoder_embed_dim', 768),\n",
    "        encoder_ffn_embed_dim=cfg_dict.get('encoder_ffn_embed_dim', 3072),\n",
    "        encoder_attention_heads=cfg_dict.get('encoder_attention_heads', 12),\n",
    "        activation_fn=cfg_dict.get('activation_fn', 'gelu'),\n",
    "        dropout=cfg_dict.get('dropout', 0.1),\n",
    "        attention_dropout=cfg_dict.get('attention_dropout', 0.1),\n",
    "        activation_dropout=cfg_dict.get('activation_dropout', 0.1),\n",
    "        final_dim=cfg_dict.get('final_dim', 1024),\n",
    "        layer_norm_first=cfg_dict.get('layer_norm_first', False),\n",
    "        conv_feature_layers=cfg_dict.get('conv_feature_layers', '[(512,10,5)]'),\n",
    "        conv_pos=cfg_dict.get('conv_pos', 128),\n",
    "        conv_pos_groups=cfg_dict.get('conv_pos_groups', 16),\n",
    "        pos_conv_depth=cfg_dict.get('pos_conv_depth', 1),\n",
    "        num_negatives=cfg_dict.get('num_negatives', 100),\n",
    "        required_seq_len_multiple=cfg_dict.get('required_seq_len_multiple', 1)\n",
    "    )\n",
    "\n",
    "    model = Wav2Vec2Model.build_model(cfg, task=None)\n",
    "\n",
    "    state_dict = checkpoint['model_weight']\n",
    "    for key in list(state_dict.keys()):\n",
    "        if 'final_proj' in key:\n",
    "            del state_dict[key]\n",
    "\n",
    "    # Load state dict in a memory-efficient way\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    # Freeze all parameters initially\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "# --------------------------\n",
    "# 5. LoRA Implementation (Optimized)\n",
    "# --------------------------\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Memory-efficient LoRA implementation\"\"\"\n",
    "    def __init__(self, original_attention, rank=4):\n",
    "        super().__init__()\n",
    "        self.original_attention = original_attention\n",
    "        self.rank = rank\n",
    "\n",
    "        # Freeze original parameters\n",
    "        for param in original_attention.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Initialize LoRA parameters more efficiently\n",
    "        # embed_dim = original_attention.embed_dim\n",
    "        # self.lora_A = nn.ParameterDict({\n",
    "        #     'q': nn.Parameter(torch.randn(rank, embed_dim) * 0.02,\n",
    "        #     'k': nn.Parameter(torch.randn(rank, embed_dim) * 0.02,\n",
    "        #     'v': nn.Parameter(torch.randn(rank, embed_dim) * 0.02\n",
    "        # })\n",
    "\n",
    "        # self.lora_B = nn.ParameterDict({\n",
    "        #     'q': nn.Parameter(torch.zeros(embed_dim, rank)),\n",
    "        #     'k': nn.Parameter(torch.zeros(embed_dim, rank)),\n",
    "        #     'v': nn.Parameter(torch.zeros(embed_dim, rank))\n",
    "        # })\n",
    "        # Initialize LoRA parameters more efficiently\n",
    "        embed_dim = original_attention.embed_dim\n",
    "        self.lora_A = nn.ParameterDict({\n",
    "            'q': nn.Parameter(torch.randn(rank, embed_dim) * 0.02),\n",
    "            'k': nn.Parameter(torch.randn(rank, embed_dim) * 0.02),\n",
    "            'v': nn.Parameter(torch.randn(rank, embed_dim) * 0.02)\n",
    "        })\n",
    "\n",
    "        self.lora_B = nn.ParameterDict({\n",
    "            'q': nn.Parameter(torch.zeros(embed_dim, rank)),\n",
    "            'k': nn.Parameter(torch.zeros(embed_dim, rank)),\n",
    "            'v': nn.Parameter(torch.zeros(embed_dim, rank))\n",
    "        })\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, need_weights=True):\n",
    "        # Original attention\n",
    "        attn_output, attn_weights = self.original_attention(\n",
    "            query, key, value,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=need_weights\n",
    "        )\n",
    "\n",
    "        # More efficient LoRA computation\n",
    "        q_lora = query @ (self.lora_A['q'].T @ self.lora_B['q'].T)\n",
    "        k_lora = key @ (self.lora_A['k'].T @ self.lora_B['k'].T)\n",
    "        v_lora = value @ (self.lora_A['v'].T @ self.lora_B['v'].T)\n",
    "\n",
    "        return attn_output + q_lora + k_lora + v_lora, attn_weights\n",
    "\n",
    "def safe_apply_lora(model, rank=4):\n",
    "    \"\"\"Apply LoRA more efficiently with memory management\"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.MultiheadAttention):\n",
    "            # Replace with optimized LoRALayer\n",
    "            new_layer = LoRALayer(module, rank).to(device)\n",
    "            setattr(model, name, new_layer)\n",
    "            # Clean up\n",
    "            del module\n",
    "            gc.collect()\n",
    "        else:\n",
    "            safe_apply_lora(module, rank)\n",
    "    return model\n",
    "\n",
    "# --------------------------\n",
    "# 6. Model Components (Optimized)\n",
    "# --------------------------\n",
    "class SpeakerHead(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=256, num_speakers=100):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_speakers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, feat_dim=256, num_classes=100, s=30.0, m=0.5):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.empty(num_classes, feat_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        W = F.normalize(self.weight, p=2, dim=1)\n",
    "\n",
    "        cosine = F.linear(embeddings, W)\n",
    "        theta = torch.acos(torch.clamp(cosine, -1+1e-7, 1-1e-7))\n",
    "\n",
    "        one_hot = F.one_hot(labels, num_classes=self.weight.size(0))\n",
    "        logits = torch.where(one_hot.bool(), theta + self.m, theta)\n",
    "        logits = torch.cos(logits) * self.s\n",
    "\n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "# --------------------------\n",
    "# 7. Optimized Training Pipeline\n",
    "# --------------------------\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for audio, labels in tqdm(train_loader, desc='Training'):\n",
    "        try:\n",
    "            audio = audio.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward pass with automatic mixed precision if on GPU\n",
    "            with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
    "                features = model(audio, features_only=True)['x']\n",
    "                logits = model.speaker_head(features)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad(set_to_none=True)  # More memory efficient\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                [p for p in model.parameters() if p.requires_grad],\n",
    "                max_norm=1.0\n",
    "            )\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * audio.size(0)\n",
    "            total_samples += audio.size(0)\n",
    "\n",
    "            # Manual memory management\n",
    "            del audio, labels, features, logits\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                print(\"Memory error, skipping batch\")\n",
    "                continue\n",
    "            raise e\n",
    "\n",
    "    return total_loss / total_samples\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize with optimizations\n",
    "        batch_size = 16 if torch.cuda.is_available() else 4\n",
    "        num_workers = min(4, os.cpu_count())\n",
    "\n",
    "        # Load model\n",
    "        print(\"Loading model...\")\n",
    "        model = load_fairseq_model('model.pt')\n",
    "        model = safe_apply_lora(model, rank=4)\n",
    "        model.speaker_head = SpeakerHead().to(device)\n",
    "\n",
    "        # Data loading with optimized settings\n",
    "        print(\"Loading data...\")\n",
    "        root_dir = 'aac'\n",
    "        if not os.path.exists(root_dir):\n",
    "            raise FileNotFoundError(f\"Dataset not found: {root_dir}\")\n",
    "\n",
    "        all_ids = sorted([d for d in os.listdir(root_dir) if d.startswith('id')])\n",
    "        if len(all_ids) < 118:\n",
    "            raise ValueError(\"Insufficient speaker IDs\")\n",
    "\n",
    "        train_ids = all_ids[:100]\n",
    "        test_ids = all_ids[100:118]\n",
    "\n",
    "        train_set = VoxCelebDataset(root_dir, train_ids)\n",
    "        test_set = VoxCelebDataset(root_dir, test_ids)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            persistent_workers=num_workers > 0\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            persistent_workers=num_workers > 0\n",
    "        )\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = ArcFaceLoss(num_classes=len(train_ids)).to(device)\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': [p for n, p in model.named_parameters() if 'lora_' in n or 'speaker_head' in n]},\n",
    "            {'params': criterion.parameters()}\n",
    "        ], lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "        # Training loop with optimizations\n",
    "        max_epochs = 1\n",
    "        for epoch in range(max_epochs):\n",
    "            try:\n",
    "                print(f\"\\nEpoch {epoch+1}/{max_epochs}\")\n",
    "\n",
    "                train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "                # Validation\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                val_samples = 0\n",
    "                with torch.no_grad():\n",
    "                    for audio, labels in test_loader:\n",
    "                        audio = audio.to(device)\n",
    "                        labels = labels.to(device)\n",
    "\n",
    "                        features = model(audio, features_only=True)['x']\n",
    "                        logits = model.speaker_head(features)\n",
    "                        val_loss += criterion(logits, labels).item() * audio.size(0)\n",
    "                        val_samples += audio.size(0)\n",
    "\n",
    "                print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss/val_samples:.4f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in epoch {epoch+1}: {str(e)}\")\n",
    "                break\n",
    "\n",
    "        # Save model\n",
    "        torch.save({\n",
    "            'model_state': model.state_dict(),\n",
    "            'speaker_head_state': model.speaker_head.state_dict(),\n",
    "            'arcface_state': criterion.state_dict()\n",
    "        }, 'fine_tuned_model.pt')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Training complete\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Starting training with optimizations...\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba3040-5ce0-4231-8899-8fb25c97d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d790e98-73cb-4519-9837-4fa277682399",
   "metadata": {},
   "source": [
    "For II Compare the performance of the pre-trained and fine-tuned model on the list of trial pairs - VoxCeleb1 (cleaned) dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7cbd11-e175-40ef-bd00-1680d40c696a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1736f35b-914e-43b9-9c04-4a4808e08949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankit/Desktop/py310venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_7563/213160517.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7563/213160517.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
      "/home/ankit/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "100%|█████████████████████████████████| 20/20 [01:48<00:00,  5.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 27.27%\n",
      "TAR@1%FAR: 0.222\n",
      "\n",
      "Evaluating: fine_tuned_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7563/213160517.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
      "/home/ankit/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/tmp/ipykernel_7563/213160517.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fine_tuned_weights = torch.load(fine_tuned_path, map_location='cpu')\n",
      "100%|█████████████████████████████████| 20/20 [01:47<00:00,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 27.27%\n",
      "TAR@1%FAR: 0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from transformers import Wav2Vec2Processor\n",
    "from fairseq.models.wav2vec.wav2vec2 import Wav2Vec2Config, Wav2Vec2Model\n",
    "\n",
    "# --- Settings ---\n",
    "TRIAL_FILE = \"VoxCeleb1.txt\"\n",
    "WAV_BASE_PATH = r\"vox1/vox1_test_wav/wav\"\n",
    "PRETRAINED_MODEL_PATH = \"model.pt\"\n",
    "FINETUNED_MODEL_PATH = \"fine_tuned_model.pt\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# CUDA config\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.max_split_size_mb = 128\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "# --- Set torchaudio backend ---\n",
    "try:\n",
    "    torchaudio.set_audio_backend(\"soundfile\")\n",
    "except:\n",
    "    print(\"Warning: torchaudio backend could not be set to 'soundfile'. Try: pip install soundfile\")\n",
    "\n",
    "# --- Processor (not used in Fairseq flow, but retained for potential future use) ---\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "\n",
    "# --- Load full fairseq model ---\n",
    "def load_fairseq_model(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    cfg_dict = checkpoint['model_cfg']\n",
    "\n",
    "    if 'final_dim' in cfg_dict and cfg_dict['final_dim'] == 768:\n",
    "        cfg_dict['final_dim'] = 1024\n",
    "\n",
    "    cfg = Wav2Vec2Config(\n",
    "        extractor_mode=cfg_dict.get('extractor_mode', 'default'),\n",
    "        encoder_layers=cfg_dict.get('encoder_layers', 12),\n",
    "        encoder_embed_dim=cfg_dict.get('encoder_embed_dim', 768),\n",
    "        encoder_ffn_embed_dim=cfg_dict.get('encoder_ffn_embed_dim', 3072),\n",
    "        encoder_attention_heads=cfg_dict.get('encoder_attention_heads', 12),\n",
    "        activation_fn=cfg_dict.get('activation_fn', 'gelu'),\n",
    "        dropout=cfg_dict.get('dropout', 0.1),\n",
    "        attention_dropout=cfg_dict.get('attention_dropout', 0.1),\n",
    "        activation_dropout=cfg_dict.get('activation_dropout', 0.1),\n",
    "        final_dim=cfg_dict.get('final_dim', 1024),\n",
    "        layer_norm_first=cfg_dict.get('layer_norm_first', False),\n",
    "        conv_feature_layers=cfg_dict.get('conv_feature_layers', '[(512,10,5)]'),\n",
    "        conv_pos=cfg_dict.get('conv_pos', 128),\n",
    "        conv_pos_groups=cfg_dict.get('conv_pos_groups', 16),\n",
    "        pos_conv_depth=cfg_dict.get('pos_conv_depth', 1),\n",
    "        num_negatives=cfg_dict.get('num_negatives', 100),\n",
    "        required_seq_len_multiple=cfg_dict.get('required_seq_len_multiple', 1)\n",
    "    )\n",
    "\n",
    "    model = Wav2Vec2Model.build_model(cfg, task=None)\n",
    "    state_dict = checkpoint['model_weight']\n",
    "\n",
    "    # Remove final projection if exists\n",
    "    for key in list(state_dict.keys()):\n",
    "        if 'final_proj' in key:\n",
    "            del state_dict[key]\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "# --- Load fine-tuned model with only state_dict ---\n",
    "def load_fine_tuned_model(fine_tuned_path, pretrained_path):\n",
    "    base_model = load_fairseq_model(pretrained_path).to(device)\n",
    "    fine_tuned_weights = torch.load(fine_tuned_path, map_location='cpu')\n",
    "    base_model.load_state_dict(fine_tuned_weights, strict=False)\n",
    "    return base_model\n",
    "\n",
    "# --- Get embedding from audio ---\n",
    "def get_embedding(wav_path, model):\n",
    "    if not os.path.exists(wav_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {wav_path}\")\n",
    "\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(wav_path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Could not load audio file: {wav_path} | Error: {e}\")\n",
    "    \n",
    "    if sr != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n",
    "\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.extract_features(waveform, padding_mask=None)\n",
    "        features = output['x'] if isinstance(output, dict) else output\n",
    "\n",
    "    return features.mean(dim=1).squeeze().cpu()\n",
    "\n",
    "# --- Cosine similarity ---\n",
    "def cosine_sim(e1, e2):\n",
    "    return F.cosine_similarity(e1.unsqueeze(0), e2.unsqueeze(0)).item()\n",
    "\n",
    "# --- Compute EER ---\n",
    "def compute_eer(y_true, scores):\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    fnr = 1 - tpr\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer * 100\n",
    "\n",
    "# --- Compute TAR@1% FAR ---\n",
    "def compute_tar_far(y_true, scores):\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    try:\n",
    "        idx = next(i for i, val in enumerate(fpr) if val > 0.01)\n",
    "        tar = tpr[idx - 1]\n",
    "    except:\n",
    "        tar = 0.0\n",
    "    return tar\n",
    "\n",
    "# --- Evaluate model on trial pairs ---\n",
    "def evaluate_model_on_trials(model_path, is_finetuned=False):\n",
    "    print(f\"\\nEvaluating: {model_path}\")\n",
    "\n",
    "    if is_finetuned:\n",
    "        model = load_fine_tuned_model(model_path, PRETRAINED_MODEL_PATH)\n",
    "    else:\n",
    "        model = load_fairseq_model(model_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings = {}\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "\n",
    "    with open(TRIAL_FILE, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in tqdm(lines):\n",
    "        try:\n",
    "            label, path1, path2 = line.strip().split()\n",
    "            label = int(label)\n",
    "\n",
    "            full_path1 = os.path.join(WAV_BASE_PATH, path1.replace('/', os.sep))\n",
    "            full_path2 = os.path.join(WAV_BASE_PATH, path2.replace('/', os.sep))\n",
    "\n",
    "            if full_path1 not in embeddings:\n",
    "                embeddings[full_path1] = get_embedding(full_path1, model)\n",
    "            if full_path2 not in embeddings:\n",
    "                embeddings[full_path2] = get_embedding(full_path2, model)\n",
    "\n",
    "            emb1 = embeddings[full_path1]\n",
    "            emb2 = embeddings[full_path2]\n",
    "\n",
    "            score = cosine_sim(emb1, emb2)\n",
    "            y_true.append(label)\n",
    "            y_score.append(score)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing pair: {line.strip()} | {e}\")\n",
    "\n",
    "    eer = compute_eer(y_true, y_score)\n",
    "    tar = compute_tar_far(y_true, y_score)\n",
    "\n",
    "    print(f\"EER: {eer:.2f}%\")\n",
    "    print(f\"TAR@1%FAR: {tar:.3f}\")\n",
    "\n",
    "evaluate_model_on_trials(PRETRAINED_MODEL_PATH, is_finetuned=False)\n",
    "evaluate_model_on_trials(FINETUNED_MODEL_PATH, is_finetuned=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5467e-3c71-424d-b6ab-240873aece58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16c6742d-cecc-4fa9-a552-8f4735117fc0",
   "metadata": {},
   "source": [
    "For III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c9003a-ae70-4c12-9bce-9d8e1fe8c873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train speakers: 100%|██████| 50/50 [29:03<00:00, 34.87s/it]\n",
      "Processing test speakers: 100%|███████| 50/50 [16:39<00:00, 19.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Cell 2: Metadata parsing function\n",
    "def parse_metadata(txt_file):\n",
    "    \"\"\"Parse VoxCeleb2 metadata from .txt files\"\"\"\n",
    "    metadata = {\n",
    "        'id': None,\n",
    "        'reference': None,\n",
    "        'offset': 0.0,\n",
    "        'duration': 0.0\n",
    "    }\n",
    "    \n",
    "    with open(txt_file, 'r') as f:\n",
    "        lines = [l.strip() for l in f.readlines()]\n",
    "        \n",
    "    for line in lines:\n",
    "        if line.startswith('Identity'):\n",
    "            metadata['id'] = line.split(':')[-1].strip()\n",
    "        elif line.startswith('Reference'):\n",
    "            metadata['reference'] = line.split(':')[-1].strip()\n",
    "        elif line.startswith('Offset'):\n",
    "            metadata['offset'] = float(line.split(':')[-1].strip())\n",
    "        elif line.startswith('FRAME'):\n",
    "            frame_count = len(lines) - lines.index(line) - 1\n",
    "            metadata['duration'] = frame_count * 0.01  # 10ms per frame\n",
    "            \n",
    "    return metadata\n",
    "\n",
    "# Cell 3: Configuration (modify these paths as needed)\n",
    "config = {\n",
    "    'aac_root': 'vox2/vox2_test_aac/aac',\n",
    "    'txt_root': 'vox2/vox2_test_txt/txt',\n",
    "    'output_wav': 'vox2/wav',\n",
    "    'output_meta_train': 'vox2/metadata/vox_metadata_train.csv',\n",
    "    'output_meta_test': 'vox2/metadata/vox_metadata_test.csv'\n",
    "}\n",
    "\n",
    "# Cell 4: Main processing function\n",
    "def prepare_voxdata(config):\n",
    "    # Create directories\n",
    "    os.makedirs(config['output_wav'], exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(config['output_meta_train']), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(config['output_meta_test']), exist_ok=True)\n",
    "    \n",
    "    # Get sorted speaker IDs\n",
    "    speaker_ids = sorted([d for d in os.listdir(config['aac_root']) \n",
    "                      if os.path.isdir(os.path.join(config['aac_root'], d))])\n",
    "    train_ids = speaker_ids[:50]  # First 50 IDs for train\n",
    "    test_ids = speaker_ids[-50:]  # Last 50 IDs for test\n",
    "    \n",
    "    # Process train and test data\n",
    "    for mode, ids in [('train', train_ids), ('test', test_ids)]:\n",
    "        output_meta = config['output_meta_train'] if mode == 'train' else config['output_meta_test']\n",
    "        \n",
    "        with open(output_meta, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['speaker_id', 'filepath', 'offset', 'duration'])\n",
    "            \n",
    "            for speaker in tqdm(ids, desc=f\"Processing {mode} speakers\"):\n",
    "                # Convert M4A to WAV\n",
    "                m4a_files = glob.glob(os.path.join(config['aac_root'], speaker, '**', '*.m4a'), \n",
    "                                    recursive=True)\n",
    "                for m4a_path in m4a_files:\n",
    "                    wav_path = m4a_path.replace(config['aac_root'], config['output_wav']).replace('.m4a', '.wav')\n",
    "                    os.makedirs(os.path.dirname(wav_path), exist_ok=True)\n",
    "                    subprocess.run([\n",
    "                        'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',\n",
    "                        '-i', m4a_path, '-ar', '16000', '-ac', '1', wav_path\n",
    "                    ])\n",
    "                    \n",
    "                # Process metadata\n",
    "                txt_files = glob.glob(os.path.join(config['txt_root'], speaker, '**', '*.txt'), \n",
    "                                 recursive=True)\n",
    "                for txt_path in txt_files:\n",
    "                    meta = parse_metadata(txt_path)\n",
    "                    if None in meta.values():\n",
    "                        continue\n",
    "                        \n",
    "                    wav_file = txt_path.replace(config['txt_root'], config['output_wav']).replace('.txt', '.wav')\n",
    "                    if os.path.exists(wav_file):\n",
    "                        writer.writerow([\n",
    "                            meta['id'],\n",
    "                            os.path.abspath(wav_file),\n",
    "                            meta['offset'],\n",
    "                            meta['duration']\n",
    "                        ])\n",
    "    print(\"Data preparation complete!\")\n",
    "\n",
    "# Cell 5: Execute the processing\n",
    "prepare_voxdata(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff63e1-fdd5-47e8-bbf9-773a5cbf0d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91fbb6c-50dc-49cb-a2e9-2f3b686a44af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train data...\n",
      "Processed 0 train mixtures\n",
      "Processed 100 train mixtures\n",
      "Processed 200 train mixtures\n",
      "Processed 300 train mixtures\n",
      "Processed 400 train mixtures\n",
      "Processed 500 train mixtures\n",
      "Processed 600 train mixtures\n",
      "Processed 700 train mixtures\n",
      "Processed 800 train mixtures\n",
      "Processed 900 train mixtures\n",
      "Processed 1000 train mixtures\n",
      "Processed 1100 train mixtures\n",
      "Processed 1200 train mixtures\n",
      "Processed 1300 train mixtures\n",
      "Processed 1400 train mixtures\n",
      "Processed 1500 train mixtures\n",
      "Processed 1600 train mixtures\n",
      "Processed 1700 train mixtures\n",
      "Processed 1800 train mixtures\n",
      "Processed 1900 train mixtures\n",
      "Processed 2000 train mixtures\n",
      "Processed 2100 train mixtures\n",
      "Processed 2200 train mixtures\n",
      "Processed 2300 train mixtures\n",
      "Processed 2400 train mixtures\n",
      "Processed 2500 train mixtures\n",
      "Processed 2600 train mixtures\n",
      "Processed 2700 train mixtures\n",
      "Processed 2800 train mixtures\n",
      "Processed 2900 train mixtures\n",
      "Processed 3000 train mixtures\n",
      "Processed 3100 train mixtures\n",
      "Processed 3200 train mixtures\n",
      "Processed 3300 train mixtures\n",
      "Processed 3400 train mixtures\n",
      "Processed 3500 train mixtures\n",
      "Processed 3600 train mixtures\n",
      "Processed 3700 train mixtures\n",
      "Processed 3800 train mixtures\n",
      "Processed 3900 train mixtures\n",
      "Processed 4000 train mixtures\n",
      "Processed 4100 train mixtures\n",
      "Processed 4200 train mixtures\n",
      "Processed 4300 train mixtures\n",
      "Processed 4400 train mixtures\n",
      "Processed 4500 train mixtures\n",
      "Processed 4600 train mixtures\n",
      "Processed 4700 train mixtures\n",
      "Processed 4800 train mixtures\n",
      "Processed 4900 train mixtures\n",
      "Processed 5000 train mixtures\n",
      "Processed 5100 train mixtures\n",
      "Processed 5200 train mixtures\n",
      "Processed 5300 train mixtures\n",
      "Processed 5400 train mixtures\n",
      "Processed 5500 train mixtures\n",
      "Processed 5600 train mixtures\n",
      "Processed 5700 train mixtures\n",
      "Processed 5800 train mixtures\n",
      "Processed 5900 train mixtures\n",
      "Processed 6000 train mixtures\n",
      "Processed 6100 train mixtures\n",
      "Processed 6200 train mixtures\n",
      "Processed 6300 train mixtures\n",
      "Processed 6400 train mixtures\n",
      "Processed 6500 train mixtures\n",
      "Processed 6600 train mixtures\n",
      "Processed 6700 train mixtures\n",
      "Processed 6800 train mixtures\n",
      "Processed 6900 train mixtures\n",
      "Processed 7000 train mixtures\n",
      "Processed 7100 train mixtures\n",
      "Processed 7200 train mixtures\n",
      "Processed 7300 train mixtures\n",
      "Processed 7400 train mixtures\n",
      "Completed processing 7489 train mixtures\n",
      "Processing test data...\n",
      "Processed 0 test mixtures\n",
      "Processed 100 test mixtures\n",
      "Processed 200 test mixtures\n",
      "Processed 300 test mixtures\n",
      "Processed 400 test mixtures\n",
      "Processed 500 test mixtures\n",
      "Processed 600 test mixtures\n",
      "Processed 700 test mixtures\n",
      "Processed 800 test mixtures\n",
      "Processed 900 test mixtures\n",
      "Processed 1000 test mixtures\n",
      "Processed 1100 test mixtures\n",
      "Processed 1200 test mixtures\n",
      "Processed 1300 test mixtures\n",
      "Processed 1400 test mixtures\n",
      "Processed 1500 test mixtures\n",
      "Processed 1600 test mixtures\n",
      "Processed 1700 test mixtures\n",
      "Processed 1800 test mixtures\n",
      "Processed 1900 test mixtures\n",
      "Processed 2000 test mixtures\n",
      "Processed 2100 test mixtures\n",
      "Processed 2200 test mixtures\n",
      "Processed 2300 test mixtures\n",
      "Processed 2400 test mixtures\n",
      "Processed 2500 test mixtures\n",
      "Processed 2600 test mixtures\n",
      "Processed 2700 test mixtures\n",
      "Processed 2800 test mixtures\n",
      "Processed 2900 test mixtures\n",
      "Processed 3000 test mixtures\n",
      "Processed 3100 test mixtures\n",
      "Processed 3200 test mixtures\n",
      "Processed 3300 test mixtures\n",
      "Processed 3400 test mixtures\n",
      "Processed 3500 test mixtures\n",
      "Processed 3600 test mixtures\n",
      "Processed 3700 test mixtures\n",
      "Processed 3800 test mixtures\n",
      "Processed 3900 test mixtures\n",
      "Processed 4000 test mixtures\n",
      "Processed 4100 test mixtures\n",
      "Processed 4200 test mixtures\n",
      "Processed 4300 test mixtures\n",
      "Processed 4400 test mixtures\n",
      "Processed 4500 test mixtures\n",
      "Processed 4600 test mixtures\n",
      "Processed 4700 test mixtures\n",
      "Processed 4800 test mixtures\n",
      "Processed 4900 test mixtures\n",
      "Processed 5000 test mixtures\n",
      "Processed 5100 test mixtures\n",
      "Processed 5200 test mixtures\n",
      "Processed 5300 test mixtures\n",
      "Processed 5400 test mixtures\n",
      "Processed 5500 test mixtures\n",
      "Processed 5600 test mixtures\n",
      "Processed 5700 test mixtures\n",
      "Processed 5800 test mixtures\n",
      "Processed 5900 test mixtures\n",
      "Processed 6000 test mixtures\n",
      "Processed 6100 test mixtures\n",
      "Processed 6200 test mixtures\n",
      "Processed 6300 test mixtures\n",
      "Processed 6400 test mixtures\n",
      "Processed 6500 test mixtures\n",
      "Processed 6600 test mixtures\n",
      "Processed 6700 test mixtures\n",
      "Processed 6800 test mixtures\n",
      "Processed 6900 test mixtures\n",
      "Processed 7000 test mixtures\n",
      "Processed 7100 test mixtures\n",
      "Processed 7200 test mixtures\n",
      "Processed 7300 test mixtures\n",
      "Processed 7400 test mixtures\n",
      "Processed 7500 test mixtures\n",
      "Processed 7600 test mixtures\n",
      "Processed 7700 test mixtures\n",
      "Processed 7800 test mixtures\n",
      "Processed 7900 test mixtures\n",
      "Processed 8000 test mixtures\n",
      "Completed processing 8010 test mixtures\n",
      "Mixture creation complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Cell 2: Configuration (modify these paths as needed)\n",
    "config = {\n",
    "    'librispeech_dir': 'vox2/wav',\n",
    "    'metadata_dir': 'vox2/metadata',\n",
    "    'librimix_outdir': 'vox_mixtures',\n",
    "    'n_src': 2,\n",
    "    'freqs': ['16k'],\n",
    "    'modes': ['max'],\n",
    "    'types': ['mix_clean']\n",
    "}\n",
    "\n",
    "# Cell 3: Main processing function\n",
    "def create_mixtures(config):\n",
    "    # Load metadata\n",
    "    meta_train = pd.read_csv(os.path.join(config['metadata_dir'], 'vox_metadata_train.csv'))\n",
    "    meta_test = pd.read_csv(os.path.join(config['metadata_dir'], 'vox_metadata_test.csv'))\n",
    "    \n",
    "    # Create output structure\n",
    "    (Path(config['librimix_outdir'])/'train').mkdir(parents=True, exist_ok=True)\n",
    "    (Path(config['librimix_outdir'])/'test').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process both train and test data\n",
    "    for mode, meta in [('train', meta_train), ('test', meta_test)]:\n",
    "        print(f\"Processing {mode} data...\")\n",
    "        mixtures = []\n",
    "        for idx in range(len(meta)//2):\n",
    "            spk1, spk2 = meta.iloc[2*idx], meta.iloc[2*idx+1]\n",
    "            \n",
    "            # Load and mix audio\n",
    "            sig1, sr = sf.read(spk1['filepath'])\n",
    "            sig2, _ = sf.read(spk2['filepath'])\n",
    "            \n",
    "            # Align lengths\n",
    "            max_len = max(len(sig1), len(sig2))\n",
    "            sig1 = np.pad(sig1, (0, max_len - len(sig1)))\n",
    "            sig2 = np.pad(sig2, (0, max_len - len(sig2)))\n",
    "            \n",
    "            # Apply SNR (0dB)\n",
    "            mixed = sig1 + sig2\n",
    "            \n",
    "            # Save files\n",
    "            mix_id = f\"mix_{idx:04d}\"\n",
    "            out_dir = Path(config['librimix_outdir'])/mode/mix_id\n",
    "            out_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            sf.write(out_dir/'mixture.wav', mixed, sr)\n",
    "            sf.write(out_dir/'s1.wav', sig1, sr)\n",
    "            sf.write(out_dir/'s2.wav', sig2, sr)\n",
    "            \n",
    "            if idx % 100 == 0:\n",
    "                print(f\"Processed {idx} {mode} mixtures\")\n",
    "        \n",
    "        print(f\"Completed processing {len(meta)//2} {mode} mixtures\")\n",
    "\n",
    "# Cell 4: Execute the processing\n",
    "create_mixtures(config)\n",
    "print(\"Mixture creation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb0e1c-43f2-4193-ae7a-4f02ae619cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020b5bc-07e4-4461-8c22-d73e81d370e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "III- A For Separation of Speakers using Sepformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971ab9a-30fa-4ea8-80e9-5b0e5910767d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5d7237-e482-48af-b3eb-3ef9ba3cdd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankit/Desktop/py310venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch masknet.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch encoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch decoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: masknet, encoder, decoder\n",
      "/home/ankit/Desktop/py310venv/lib/python3.10/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "Processing mixtures:   1%|       | 53/8010 [10:20<25:51:51, 11.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m output_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvox_separated_8k\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Output directory for 8kHz files\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Run separation\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43mseparate_mixtures\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_root\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m, in \u001b[0;36mseparate_mixtures\u001b[0;34m(input_root, output_root)\u001b[0m\n\u001b[1;32m     43\u001b[0m resample_to_8k(mix_path, mix_8k_path)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# 2. Separate at 8kHz\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m est_sources \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseparate_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmix_8k_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 3. Save separated files at 8kHz\u001b[39;00m\n\u001b[1;32m     49\u001b[0m torchaudio\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mstr\u001b[39m(sep1_8k_path),\n\u001b[1;32m     51\u001b[0m     est_sources[:, :, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;241m8000\u001b[39m\n\u001b[1;32m     53\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/speechbrain/inference/separation.py:124\u001b[0m, in \u001b[0;36mSepformerSeparation.separate_file\u001b[0;34m(self, path, savedir)\u001b[0m\n\u001b[1;32m    121\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    122\u001b[0m     batch \u001b[38;5;241m=\u001b[39m tf(batch)\n\u001b[0;32m--> 124\u001b[0m est_sources \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseparate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m est_sources \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    126\u001b[0m     est_sources \u001b[38;5;241m/\u001b[39m est_sources\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    127\u001b[0m )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m est_sources\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/speechbrain/inference/separation.py:62\u001b[0m, in \u001b[0;36mSepformerSeparation.separate_batch\u001b[0;34m(self, mix)\u001b[0m\n\u001b[1;32m     60\u001b[0m mix \u001b[38;5;241m=\u001b[39m mix\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     61\u001b[0m mix_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mencoder(mix)\n\u001b[0;32m---> 62\u001b[0m est_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasknet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmix_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m mix_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([mix_w] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mnum_spks)\n\u001b[1;32m     64\u001b[0m sep_h \u001b[38;5;241m=\u001b[39m mix_w \u001b[38;5;241m*\u001b[39m est_mask\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/speechbrain/lobes/models/dual_path.py:1067\u001b[0m, in \u001b[0;36mDual_Path_Model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;66;03m# [B, N, K, S]\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m-> 1067\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual_mdl\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprelu(x)\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# [B, N*spks, K, S]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/speechbrain/lobes/models/dual_path.py:897\u001b[0m, in \u001b[0;36mDual_Computation_Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    894\u001b[0m intra \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B \u001b[38;5;241m*\u001b[39m S, K, N)\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# [BS, K, H]\u001b[39;00m\n\u001b[0;32m--> 897\u001b[0m intra \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintra_mdl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintra\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# [BS, K, N]\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layer_after_inter_intra:\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/speechbrain/lobes/models/dual_path.py:638\u001b[0m, in \u001b[0;36mSBTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_positional_encoding:\n\u001b[1;32m    637\u001b[0m     pos_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_enc(x)\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmdl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpos_enc\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmdl(x)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/speechbrain/lobes/models/transformer/Transformer.py:622\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, pos_embs, dynchunktrain_config)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, enc_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop_prob \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m keep_probs[i] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop_prob\n\u001b[1;32m    621\u001b[0m     ):\n\u001b[0;32m--> 622\u001b[0m         output, attention \u001b[38;5;241m=\u001b[39m \u001b[43menc_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpos_embs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_embs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m         attention_lst\u001b[38;5;241m.\u001b[39mappend(attention)\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_hidden_states:\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/speechbrain/lobes/models/transformer/Transformer.py:458\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, pos_embs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     src1 \u001b[38;5;241m=\u001b[39m src\n\u001b[0;32m--> 458\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_ffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# add & norm\u001b[39;00m\n\u001b[1;32m    461\u001b[0m output \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(output)\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/speechbrain/nnet/attention.py:935\u001b[0m, in \u001b[0;36mPositionalwiseFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# give a tensor of shape (time, batch, fea)\u001b[39;00m\n\u001b[1;32m    934\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 935\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# reshape the output back to (batch, time, fea)\u001b[39;00m\n\u001b[1;32m    938\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/py310venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "from speechbrain.inference.separation import SepformerSeparation as separator\n",
    "from tqdm import tqdm\n",
    "\n",
    "def resample_to_8k(input_path, output_path):\n",
    "    \"\"\"Convert audio file to 8kHz using torchaudio\"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(input_path)\n",
    "    if sample_rate != 8000:\n",
    "        resampler = torchaudio.transforms.Resample(\n",
    "            orig_freq=sample_rate, \n",
    "            new_freq=8000\n",
    "        )\n",
    "        waveform = resampler(waveform)\n",
    "    torchaudio.save(output_path, waveform, 8000)\n",
    "\n",
    "def separate_mixtures(input_root, output_root):\n",
    "    # Initialize SepFormer model\n",
    "    model = separator.from_hparams(\n",
    "        source=\"speechbrain/sepformer-whamr\",\n",
    "        savedir='pretrained_models/sepformer-whamr'\n",
    "    )\n",
    "    \n",
    "    # Create output directory\n",
    "    Path(output_root).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find all mixture.wav files\n",
    "    mixture_files = list(Path(input_root).glob('**/mixture.wav'))\n",
    "    \n",
    "    for mix_path in tqdm(mixture_files, desc=\"Processing mixtures\"):\n",
    "        # Create corresponding output directory\n",
    "        rel_path = mix_path.relative_to(input_root).parent\n",
    "        output_dir = Path(output_root) / rel_path\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Paths for 8kHz files\n",
    "        mix_8k_path = output_dir / \"mixture.wav\"\n",
    "        sep1_8k_path = output_dir / \"sep_s1.wav\"\n",
    "        sep2_8k_path = output_dir / \"sep_s2.wav\"\n",
    "        \n",
    "        # 1. Convert mixture to 8kHz and save\n",
    "        resample_to_8k(mix_path, mix_8k_path)\n",
    "        \n",
    "        # 2. Separate at 8kHz\n",
    "        est_sources = model.separate_file(path=str(mix_8k_path))\n",
    "        \n",
    "        # 3. Save separated files at 8kHz\n",
    "        torchaudio.save(\n",
    "            str(sep1_8k_path),\n",
    "            est_sources[:, :, 0].detach().cpu(),\n",
    "            8000\n",
    "        )\n",
    "        torchaudio.save(\n",
    "            str(sep2_8k_path),\n",
    "            est_sources[:, :, 1].detach().cpu(),\n",
    "            8000\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure paths\n",
    "    input_root = \"vox_mixtures/test\"  # Input directory with original mixtures\n",
    "    output_root = \"vox_separated_8k\"  # Output directory for 8kHz files\n",
    "    \n",
    "    # Run separation\n",
    "    separate_mixtures(input_root, output_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d001ca8-3dfa-4eda-adaa-6d40c0aba69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df6e33-67ca-4507-b1e2-d455d59ae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "III- A For Enhancement of Speakers using Sepformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6672c933-614c-4daf-b12e-c0d5f50ff9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7899d363-3cf8-4c49-81be-df5b6bd3e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham-enhancement' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham-enhancement' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch encoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham-enhancement' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch masknet.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham-enhancement' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch decoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham-enhancement' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: encoder, masknet, decoder\n",
      "Processing:   0%|                             | 0/106 [00:00<?, ?it/s]/tmp/ipykernel_7627/3007529742.py:58: FutureWarning: mir_eval.separation.bss_eval_sources\n",
      "\tDeprecated as of mir_eval version 0.8.\n",
      "\tIt will be removed in mir_eval version 0.9.\n",
      "  sdr, _, sar, _ = bss_eval_sources(orig[np.newaxis,:], enh[np.newaxis,:])\n",
      "Processing: 100%|███████████████████| 106/106 [28:16<00:00, 16.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Results saved to vox_enhanced_8k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from speechbrain.inference.separation import SepformerSeparation as separator\n",
    "from pesq import pesq\n",
    "from mir_eval.separation import bss_eval_sources\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import csv\n",
    "\n",
    "def validate_audio(waveform):\n",
    "    \"\"\"Audio validation with phase preservation\"\"\"\n",
    "    if torch.max(torch.abs(waveform)) < 0.01:\n",
    "        raise ValueError(\"Silent audio detected\")\n",
    "    if len(torch.unique(waveform)) < 100:\n",
    "        raise ValueError(\"Audio contains artifacts\")\n",
    "    return waveform / (torch.max(torch.abs(waveform)) + 1e-7)\n",
    "\n",
    "def correct_whamr_phase(signal):\n",
    "    \"\"\"Correct WHAMR's automatic phase inversion\"\"\"\n",
    "    return -signal  # WHAMR outputs are phase-inverted\n",
    "\n",
    "def calculate_metrics(original, enhanced, sr=8000):\n",
    "    \"\"\"Revised metric calculation for meaningful SIR values\"\"\"\n",
    "    metrics = {'sir': 0.0, 'sar': 0.0, 'sdr': 0.0, 'pesq': 0.0}\n",
    "    \n",
    "    try:\n",
    "        # Convert and match lengths\n",
    "        orig = original.numpy().squeeze().astype(np.float64)\n",
    "        enh = enhanced.numpy().squeeze().astype(np.float64)\n",
    "        min_len = min(len(orig), len(enh))\n",
    "        orig = orig[:min_len]\n",
    "        enh = enh[:min_len]\n",
    "\n",
    "        # Phase correction\n",
    "        corr = np.corrcoef(orig, enh)[0,1]\n",
    "        enh = -enh if corr < -0.8 else enh  # Only flip if strongly anti-correlated\n",
    "        \n",
    "        # Normalization with dither\n",
    "        orig = orig / (np.max(np.abs(orig)) + 1e-7) + np.random.normal(0, 1e-10, min_len)\n",
    "        enh = enh / (np.max(np.abs(enh)) + 1e-7) + np.random.normal(0, 1e-10, min_len)\n",
    "        \n",
    "        # NEW SIR CALCULATION METHOD\n",
    "        # 1. Calculate residual (what was removed by enhancement)\n",
    "        residual = orig - enh\n",
    "        \n",
    "        # 2. Calculate power ratios\n",
    "        signal_power = np.mean(enh**2)\n",
    "        interference_power = np.mean(residual**2)\n",
    "        \n",
    "        # 3. Compute SIR directly in dB\n",
    "        with np.errstate(divide='ignore'):\n",
    "            sir_db = 10 * np.log10(signal_power / (interference_power + 1e-10))\n",
    "        \n",
    "        # Traditional BSS metrics for others\n",
    "        sdr, _, sar, _ = bss_eval_sources(orig[np.newaxis,:], enh[np.newaxis,:])\n",
    "        \n",
    "        metrics['sir'] = float(np.clip(sir_db, -5, 25))  # Wider realistic range\n",
    "        metrics['sar'] = float(np.clip(sar[0], 0, 25))\n",
    "        metrics['sdr'] = float(np.clip(sdr[0], -5, 25))\n",
    "        \n",
    "        # PESQ calculation\n",
    "        if min_len >= 2400:\n",
    "            metrics['pesq'] = float(np.clip(pesq(sr, orig, enh, 'nb'), 1.0, 4.5))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Metric calculation warning: {str(e)}\")\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "def enhance_audio(model, input_path, output_dir):\n",
    "    \"\"\"Enhancement with WHAMR phase handling\"\"\"\n",
    "    try:\n",
    "        # Create output directory\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Load and validate\n",
    "        waveform, sr = torchaudio.load(input_path)\n",
    "        if sr != 8000:\n",
    "            raise ValueError(f\"Expected 8kHz audio, got {sr}Hz\")\n",
    "        \n",
    "        waveform = validate_audio(waveform)\n",
    "        \n",
    "        # Save original (with phase correction)\n",
    "        spk = input_path.stem.split('_')[-1]\n",
    "        orig_path = output_dir / f\"orig_{spk}.wav\"\n",
    "        corrected_orig = correct_whamr_phase(waveform.numpy())\n",
    "        sf.write(orig_path, corrected_orig.squeeze(), 8000, subtype='PCM_16')\n",
    "        \n",
    "        # Enhance\n",
    "        enhanced = model.separate_file(path=str(input_path))\n",
    "        enhanced = enhanced[:,:,0].detach().cpu()\n",
    "        enhanced = validate_audio(enhanced)\n",
    "        \n",
    "        # Save enhanced\n",
    "        enh_path = output_dir / f\"enhanced_{spk}.wav\"\n",
    "        sf.write(enh_path, enhanced.squeeze().numpy(), 8000, subtype='PCM_16')\n",
    "        \n",
    "        return enhanced\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Processing failed for {input_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_directory(input_root, output_root):\n",
    "    \"\"\"Main processing pipeline\"\"\"\n",
    "    try:\n",
    "        model = separator.from_hparams(\n",
    "            source=\"speechbrain/sepformer-wham-enhancement\",\n",
    "            savedir='pretrained_models/sepformer-wham-enhancement'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Model initialization failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Setup output\n",
    "    Path(output_root).mkdir(parents=True, exist_ok=True)\n",
    "    csv_path = Path(output_root) / \"enhancement_metrics.csv\"\n",
    "    \n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['mix_id', 'speaker', 'sir_db', 'sar_db', 'sdr_db', 'pesq'])\n",
    "    \n",
    "    # Process all speaker files\n",
    "    speaker_files = []\n",
    "    for root, _, files in os.walk(input_root):\n",
    "        for file in files:\n",
    "            if file.startswith('sep_') and file.endswith('.wav'):\n",
    "                speaker_files.append(Path(root) / file)\n",
    "    \n",
    "    for input_path in tqdm(speaker_files, desc=\"Processing\"):\n",
    "        mix_id = input_path.parent.name\n",
    "        spk = input_path.stem.split('_')[-1]\n",
    "        \n",
    "        enhanced_dir = Path(output_root) / mix_id\n",
    "        enhanced = enhance_audio(model, input_path, enhanced_dir)\n",
    "        \n",
    "        if enhanced is not None:\n",
    "            try:\n",
    "                original, _ = torchaudio.load(input_path)\n",
    "                metrics = calculate_metrics(original, enhanced)\n",
    "                \n",
    "                with open(csv_path, 'a', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow([\n",
    "                        mix_id,\n",
    "                        spk,\n",
    "                        f\"{metrics['sir']:.2f}\",\n",
    "                        f\"{metrics['sar']:.2f}\",\n",
    "                        f\"{metrics['sdr']:.2f}\",\n",
    "                        f\"{metrics['pesq']:.2f}\"\n",
    "                    ])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Metrics failed for {input_path}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_base = \"vox_separated_8k\"  # Contains sep_s1.wav, sep_s2.wav\n",
    "    output_base = \"vox_enhanced_8k\"\n",
    "    \n",
    "    process_directory(input_base, output_base)\n",
    "    print(f\"Processing complete. Results saved to {output_base}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1702c41e-7309-49d9-8c7b-a4ed4f08145d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2c53b-8c48-4b5b-99e7-def3247ef64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "III- B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ddc65f-62b6-4ef2-9b6c-7caeb4e24a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64158b8a-fd17-4407-98b9-6c81604bb7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting speaker identification evaluation...\n",
      "Found 106 enhanced audio files for evaluation\n",
      "\n",
      "Loading pre-trained model...\n",
      "\n",
      "Loading fine-tuned model...\n",
      "\n",
      "Extracting embeddings with Pre-trained HuBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|█████████████| 106/106 [04:14<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Pre-trained HuBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating similarities: 100%|███| 106/106 [00:00<00:00, 1659.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained HuBERT Rank-1 Accuracy: 36.79%\n",
      "\n",
      "Extracting embeddings with Fine-tuned HuBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|█████████████| 106/106 [04:19<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Fine-tuned HuBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating similarities: 100%|███| 106/106 [00:00<00:00, 2160.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned HuBERT Rank-1 Accuracy: 42.45%\n",
      "\n",
      "Results saved to speaker_identification_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import io\n",
    "import csv\n",
    "import torchaudio\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# --------------------------\n",
    "# 1. Configuration for Optimization\n",
    "# --------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_num_threads(os.cpu_count()) if device.type == 'cpu' else None\n",
    "os.environ['OMP_NUM_THREADS'] = str(os.cpu_count())\n",
    "os.environ['MKL_NUM_THREADS'] = str(os.cpu_count())\n",
    "\n",
    "# --------------------------\n",
    "# 2. Memory-Efficient Audio Loading\n",
    "# --------------------------\n",
    "def load_audio(path, sample_rate=16000):\n",
    "    \"\"\"Optimized audio loading that minimizes disk usage\"\"\"\n",
    "    try:\n",
    "        # First try direct loading for supported formats\n",
    "        if path.lower().endswith(('.wav', '.flac')):\n",
    "            audio, _ = sf.read(path, dtype='float32', always_2d=False)\n",
    "        else:\n",
    "            # Use in-memory conversion for unsupported formats\n",
    "            cmd = [\n",
    "                'ffmpeg', '-y', '-i', path,\n",
    "                '-ac', '1', '-ar', str(sample_rate),\n",
    "                '-f', 'wav', '-'\n",
    "            ]\n",
    "            process = subprocess.Popen(\n",
    "                cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
    "                stdin=subprocess.DEVNULL\n",
    "            )\n",
    "            stdout, _ = process.communicate()\n",
    "            \n",
    "            # Read directly from memory\n",
    "            audio, _ = sf.read(io.BytesIO(stdout), dtype='float32')\n",
    "\n",
    "        if len(audio) == 0 or np.max(np.abs(audio)) < 0.001:\n",
    "            return np.zeros(sample_rate * 3, dtype=np.float32)\n",
    "\n",
    "        return audio.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {str(e)}\")\n",
    "        return np.zeros(sample_rate * 3, dtype=np.float32)\n",
    "\n",
    "# --------------------------\n",
    "# 3. Speaker Identification Model\n",
    "# --------------------------\n",
    "class HuBERTSpeakerIdentifier:\n",
    "    def __init__(self, model_path, fine_tuned=False):\n",
    "        self.device = device\n",
    "        self.fine_tuned = fine_tuned\n",
    "        \n",
    "        # Load feature extractor\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-large-ll60k\")\n",
    "        \n",
    "        # Load model architecture\n",
    "        self.model = HubertModel.from_pretrained(\"facebook/hubert-large-ll60k\")\n",
    "        # Enable output of hidden states\n",
    "        self.model.config.output_hidden_states = True\n",
    "        \n",
    "        # Load custom weights with security check\n",
    "        if model_path:\n",
    "            try:\n",
    "                state_dict = torch.load(model_path, map_location='cpu', weights_only=True)\n",
    "            except:\n",
    "                state_dict = torch.load(model_path, map_location='cpu')\n",
    "            \n",
    "            model_state_dict = state_dict.get('model', state_dict)\n",
    "            model_state_dict = {k.replace('module.', ''): v for k, v in model_state_dict.items()}\n",
    "            self.model.load_state_dict(model_state_dict, strict=False)\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def extract_embedding(self, audio_path):\n",
    "        try:\n",
    "            # Load and validate audio\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if waveform.nelement() == 0:\n",
    "                raise ValueError(\"Empty audio file\")\n",
    "                \n",
    "            if sr != 16000:\n",
    "                waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "            \n",
    "            if waveform.shape[1] < 16000:\n",
    "                raise ValueError(\"Audio too short\")\n",
    "            \n",
    "            # Extract features and move them to device\n",
    "            inputs = self.feature_extractor(\n",
    "                waveform.squeeze().numpy(),\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            if torch.isnan(inputs['input_values']).any():\n",
    "                raise ValueError(\"NaN values in input\")\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            if self.fine_tuned:\n",
    "                embeddings = outputs.last_hidden_state\n",
    "            else:\n",
    "                hidden_states = outputs.hidden_states\n",
    "                # Ensure hidden_states is not None\n",
    "                if hidden_states is None:\n",
    "                    raise ValueError(\"Model did not return hidden states\")\n",
    "                selected_layers = hidden_states[6:13]\n",
    "                embeddings = torch.mean(torch.stack(selected_layers), dim=0)\n",
    "            \n",
    "            if embeddings.ndim == 3:\n",
    "                embeddings = embeddings.mean(dim=1)\n",
    "            return embeddings.squeeze(0).cpu().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Skipped {audio_path}: {str(e)}\")\n",
    "            return None\n",
    "# --------------------------\n",
    "# 4. Evaluation Pipeline\n",
    "# --------------------------\n",
    "def collect_enhanced_audio_files(root_dir):\n",
    "    \"\"\"Collect all enhanced audio files and their speaker labels\"\"\"\n",
    "    audio_files = []\n",
    "    speaker_labels = []\n",
    "    \n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.startswith('enhanced_') and file.endswith('.wav'):\n",
    "                speaker = file.split('_')[-1].split('.')[0]\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "                speaker_labels.append(speaker)\n",
    "    \n",
    "    return audio_files, speaker_labels\n",
    "\n",
    "def evaluate_speaker_identification(audio_files, speaker_labels, identifier, model_name):\n",
    "    \"\"\"Enhanced evaluation with progress tracking\"\"\"\n",
    "    print(f\"\\nExtracting embeddings with {model_name}...\")\n",
    "    embeddings = []\n",
    "    valid_labels = []\n",
    "    \n",
    "    for audio_path, label in tqdm(zip(audio_files, speaker_labels), \n",
    "                                total=len(audio_files),\n",
    "                                desc=\"Processing files\"):\n",
    "        emb = identifier.extract_embedding(audio_path)\n",
    "        if emb is not None:\n",
    "            embeddings.append(emb)\n",
    "            valid_labels.append(label)\n",
    "    \n",
    "    if not embeddings:\n",
    "        print(\"No valid embeddings extracted\")\n",
    "        return 0.0\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    embeddings = np.array(embeddings)\n",
    "    labels = np.array(valid_labels)\n",
    "    \n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    correct = 0\n",
    "    \n",
    "    for i in tqdm(range(len(embeddings)), desc=\"Calculating similarities\"):\n",
    "        similarities = cosine_similarity([embeddings[i]], embeddings)[0]\n",
    "        similarities[i] = -np.inf  # Exclude self\n",
    "        predicted_index = np.argmax(similarities)\n",
    "        if labels[predicted_index] == labels[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / len(embeddings)\n",
    "    print(f\"{model_name} Rank-1 Accuracy: {accuracy:.2%}\")\n",
    "    return accuracy\n",
    "\n",
    "# --------------------------\n",
    "# 5. Main Execution\n",
    "# --------------------------\n",
    "def main():\n",
    "    # Path configurations\n",
    "    enhanced_audio_dir = \"vox_enhanced_8k\"\n",
    "    pretrained_model_path = \"model.pt\"  # Original HuBERT large\n",
    "    finetuned_model_path = \"fine_tuned_model.pt\"  # Fine-tuned on VoxCeleb\n",
    "    results_file = \"speaker_identification_results.csv\"\n",
    "    \n",
    "    # Collect enhanced audio files\n",
    "    audio_files, speaker_labels = collect_enhanced_audio_files(enhanced_audio_dir)\n",
    "    if not audio_files:\n",
    "        print(\"No enhanced audio files found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(audio_files)} enhanced audio files for evaluation\")\n",
    "    \n",
    "    # Initialize identifiers\n",
    "    print(\"\\nLoading pre-trained model...\")\n",
    "    pretrained_identifier = HuBERTSpeakerIdentifier(pretrained_model_path, fine_tuned=False)\n",
    "    \n",
    "    print(\"\\nLoading fine-tuned model...\")\n",
    "    finetuned_identifier = HuBERTSpeakerIdentifier(finetuned_model_path, fine_tuned=True)\n",
    "    \n",
    "    # Evaluate both models\n",
    "    pretrained_acc = evaluate_speaker_identification(\n",
    "        audio_files, speaker_labels, pretrained_identifier, \"Pre-trained HuBERT\"\n",
    "    )\n",
    "    \n",
    "    finetuned_acc = evaluate_speaker_identification(\n",
    "        audio_files, speaker_labels, finetuned_identifier, \"Fine-tuned HuBERT\"\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    with open(results_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Model\", \"Rank-1 Accuracy\"])\n",
    "        writer.writerow([\"Pre-trained HuBERT\", f\"{pretrained_acc:.4f}\"])\n",
    "        writer.writerow([\"Fine-tuned HuBERT\", f\"{finetuned_acc:.4f}\"])\n",
    "    \n",
    "    print(f\"\\nResults saved to {results_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting speaker identification evaluation...\")\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9685b-bf60-49d9-826a-1d6106dd275c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c177f694-d060-4b01-9c28-1d35760565ed",
   "metadata": {},
   "source": [
    "IV Pipeline Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a36aee0-6883-4806-961b-db7a5d1f1a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankit/Desktop/py310venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch masknet.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch encoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch decoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: masknet, encoder, decoder\n",
      "/home/ankit/Desktop/py310venv/lib/python3.10/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/tmp/ipykernel_7688/2022823716.py:226: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/home/ankit/Desktop/py310venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Epoch 1:   0%|                                                                                                          | 0/17 [00:00<?, ?it/s]/home/ankit/Desktop/py310venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [03:30<00:00, 12.39s/it]\n",
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [02:59<00:00, 10.54s/it]\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [03:46<00:00, 13.30s/it]\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [04:12<00:00, 14.86s/it]\n",
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [03:23<00:00, 11.97s/it]\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch masknet.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch encoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch decoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-whamr' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: masknet, encoder, decoder\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham-enhancement' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham-enhancement' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch encoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham-enhancement' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch masknet.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham-enhancement' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch decoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham-enhancement' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: encoder, masknet, decoder\n",
      "/tmp/ipykernel_7688/2022823716.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(hubert_path, map_location='cpu')\n",
      "Processing samples:   0%|                                                                                               | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_7688/2022823716.py:111: FutureWarning: mir_eval.separation.bss_eval_sources\n",
      "\tDeprecated as of mir_eval version 0.8.\n",
      "\tIt will be removed in mir_eval version 0.9.\n",
      "  sdr, sir, sar, _ = bss_eval_sources(\n",
      "Processing samples: 100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [1:23:40<00:00, 251.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation Results:\n",
      "SDR: -3.02 dB\n",
      "SIR: nan dB\n",
      "SAR: -3.02 dB\n",
      "PESQ: 1.31\n",
      "Rank-1 Accuracy (Pretrained): 60.00%\n",
      "Rank-1 Accuracy (Finetuned): 60.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pesq import pesq\n",
    "from mir_eval.separation import bss_eval_sources\n",
    "import csv\n",
    "from speechbrain.inference.separation import SepformerSeparation as Separator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import HubertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    train_data_dir = \"vox_mixtures/train\"\n",
    "    test_data_dir = \"vox_mixtures/test\"\n",
    "    batch_size = 2\n",
    "    sample_rate = 8000\n",
    "    num_epochs = 5  \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Model configurations\n",
    "    sep_model_source = \"speechbrain/sepformer-whamr\"\n",
    "    sep_model_savedir = \"pretrained_models/sepformer-whamr\"\n",
    "    enh_model_source = \"speechbrain/sepformer-wham-enhancement\"\n",
    "    enh_model_savedir = \"pretrained_models/sepformer-wham-enhancement\"\n",
    "    hubert_pretrained = \"model.pt\"\n",
    "    hubert_finetuned = \"fine_tuned_model.pt\"\n",
    "\n",
    "# Dataset Loader\n",
    "class MultiSpeakerDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.samples = []\n",
    "        for mix_dir in sorted(os.listdir(root_dir)):\n",
    "            mix_path = os.path.join(root_dir, mix_dir, \"mixture.wav\")\n",
    "            s1_path = os.path.join(root_dir, mix_dir, \"s1.wav\")\n",
    "            s2_path = os.path.join(root_dir, mix_dir, \"s2.wav\")\n",
    "            \n",
    "            if all(os.path.exists(p) for p in [mix_path, s1_path, s2_path]):\n",
    "                self.samples.append({\n",
    "                    'mix': mix_path,\n",
    "                    's1': s1_path,\n",
    "                    's2': s2_path,\n",
    "                    'mix_id': mix_dir\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        def load_audio(path):\n",
    "            waveform, sr = torchaudio.load(path)\n",
    "            waveform = waveform.mean(dim=0) if waveform.dim() > 1 else waveform\n",
    "            waveform = waveform.squeeze().contiguous()\n",
    "            if sr != Config.sample_rate:\n",
    "                waveform = torchaudio.functional.resample(\n",
    "                    waveform, \n",
    "                    orig_freq=sr, \n",
    "                    new_freq=Config.sample_rate\n",
    "                )\n",
    "            return waveform\n",
    "        \n",
    "        return {\n",
    "            'mix': load_audio(sample['mix']),\n",
    "            's1': load_audio(sample['s1']),\n",
    "            's2': load_audio(sample['s2']),\n",
    "            'mix_id': sample['mix_id']\n",
    "        }\n",
    "\n",
    "def custom_collate(batch):\n",
    "    max_len = max([x['mix'].shape[-1] for x in batch])\n",
    "    \n",
    "    def pad(item):\n",
    "        return F.pad(item, (0, max_len - item.shape[-1]))\n",
    "    \n",
    "    return {\n",
    "        'mix': torch.stack([pad(x['mix']) for x in batch]),\n",
    "        's1': torch.stack([pad(x['s1']) for x in batch]),\n",
    "        's2': torch.stack([pad(x['s2']) for x in batch]),\n",
    "        'mix_id': [x['mix_id'] for x in batch]\n",
    "    }\n",
    "\n",
    "# Improved Metric Calculation\n",
    "def calculate_metrics(clean, enhanced, sr=8000):\n",
    "    metrics = {'sdr': np.nan, 'sir': np.nan, 'sar': np.nan, 'pesq': np.nan}\n",
    "    clean = clean.numpy().astype(np.float64)\n",
    "    enhanced = enhanced.numpy().astype(np.float64)\n",
    "    \n",
    "    if len(clean) == 0 or len(enhanced) == 0:\n",
    "        return metrics\n",
    "    \n",
    "    # Phase alignment\n",
    "    try:\n",
    "        corr = np.correlate(clean, enhanced, mode='full')\n",
    "        lag = np.argmax(corr) - (len(enhanced)-1)\n",
    "        enhanced = np.roll(enhanced, lag)\n",
    "    except:\n",
    "        return metrics\n",
    "    \n",
    "    # Trim to same length\n",
    "    min_len = min(len(clean), len(enhanced))\n",
    "    clean = clean[:min_len]\n",
    "    enhanced = enhanced[:min_len]\n",
    "    \n",
    "    # BSS Eval\n",
    "    try:\n",
    "        sdr, sir, sar, _ = bss_eval_sources(\n",
    "            clean[np.newaxis, :], \n",
    "            enhanced[np.newaxis, :],\n",
    "            compute_permutation=False\n",
    "        )\n",
    "        metrics['sdr'] = sdr[0]\n",
    "        metrics['sir'] = sir[0]\n",
    "        metrics['sar'] = sar[0]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    # PESQ\n",
    "    try:\n",
    "        metrics['pesq'] = pesq(sr, clean, enhanced, 'nb')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Replace inf/nan\n",
    "    for k in metrics:\n",
    "        if np.isinf(metrics[k]) or np.isnan(metrics[k]):\n",
    "            metrics[k] = np.nan\n",
    "            \n",
    "    return metrics\n",
    "\n",
    "# Improved Speaker Identifier\n",
    "class SpeakerIdentifier(nn.Module):\n",
    "    def __init__(self, hubert_path, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = HubertModel.from_pretrained(\"facebook/hubert-large-ll60k\")\n",
    "        \n",
    "        try:\n",
    "            state_dict = torch.load(hubert_path, map_location='cpu')\n",
    "            self.model.load_state_dict(state_dict, strict=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading HuBERT weights: {str(e)}\")\n",
    "            \n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def get_embedding(self, audio):\n",
    "        # Resample to 16kHz for HuBERT\n",
    "        audio = audio.cpu()\n",
    "        audio_16k = torchaudio.functional.resample(\n",
    "            audio, \n",
    "            orig_freq=Config.sample_rate,\n",
    "            new_freq=16000\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_values': audio_16k.unsqueeze(0).to(self.device),\n",
    "                'attention_mask': torch.ones_like(audio_16k).unsqueeze(0).to(self.device)\n",
    "            }\n",
    "            outputs = self.model(**inputs)\n",
    "            return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "def si_snr_loss(est, target, epsilon=1e-8):\n",
    "    target_zero_mean = target - torch.mean(target, dim=-1, keepdim=True)\n",
    "    est_zero_mean = est - torch.mean(est, dim=-1, keepdim=True)\n",
    "\n",
    "    power_target = torch.sum(target_zero_mean ** 2, dim=-1) + epsilon\n",
    "    alpha = torch.sum(est_zero_mean * target_zero_mean, dim=-1) / power_target\n",
    "\n",
    "    target_component = alpha.unsqueeze(-1) * target_zero_mean\n",
    "    noise_component = est_zero_mean - target_component\n",
    "\n",
    "    power_target = torch.sum(target_component ** 2, dim=-1) + epsilon\n",
    "    power_noise = torch.sum(noise_component ** 2, dim=-1) + epsilon\n",
    "\n",
    "    si_snr = 10 * torch.log10(power_target / power_noise)\n",
    "    return -torch.mean(si_snr)\n",
    "\n",
    "# Enhanced Training Loop\n",
    "def train_sepformer():\n",
    "    config = Config()\n",
    "    device = torch.device(config.device)\n",
    "\n",
    "    class SafeDataset(MultiSpeakerDataset):\n",
    "        def __getitem__(self, idx):\n",
    "            sample = super().__getitem__(idx)\n",
    "            max_length = 16000 * 2  # 2-second clips\n",
    "            return {k: v[:max_length] if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in sample.items()}\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        SafeDataset(config.train_data_dir),\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    sep_model = Separator.from_hparams(\n",
    "        source=config.sep_model_source,\n",
    "        savedir=config.sep_model_savedir,\n",
    "        run_opts={\"device\": device}\n",
    "    ).to(device)\n",
    "\n",
    "    class CheckpointedEncoder(nn.Module):\n",
    "        def __init__(self, original_encoder):\n",
    "            super().__init__()\n",
    "            self.encoder = original_encoder\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return torch.utils.checkpoint.checkpoint(\n",
    "                self._forward_impl, x, use_reentrant=False)\n",
    "            \n",
    "        def _forward_impl(self, x):\n",
    "            x = self.encoder.conv1d(x)\n",
    "            return torch.relu(x)\n",
    "\n",
    "    sep_model.mods.encoder = CheckpointedEncoder(sep_model.mods.encoder)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    optimizer = torch.optim.Adam(sep_model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        sep_model.train()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                mix = batch['mix'].unsqueeze(1).to(device)\n",
    "                s1 = batch['s1'].unsqueeze(1).to(device)\n",
    "                s2 = batch['s2'].unsqueeze(1).to(device)\n",
    "\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    est_sources = sep_model.separate_batch(mix)\n",
    "                    loss_s1 = si_snr_loss(est_sources[:, 0].squeeze(1), s1.squeeze(1))\n",
    "                    loss_s2 = si_snr_loss(est_sources[:, 1].squeeze(1), s2.squeeze(1))\n",
    "                    loss = (loss_s1 + loss_s2) / 2\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(sep_model.parameters(), 5.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                del est_sources, mix, s1, s2\n",
    "                progress_bar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if 'CUDA out of memory' in str(e):\n",
    "                    print(\"\\nSkipping batch due to OOM\")\n",
    "                    optimizer.zero_grad()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "    torch.save(sep_model.state_dict(), \"sepformer_trained.pt\")\n",
    "\n",
    "# Enhanced Evaluation Pipeline\n",
    "def evaluate_pipeline():\n",
    "    config = Config()\n",
    "    test_dataset = MultiSpeakerDataset(config.test_data_dir)\n",
    "    \n",
    "    sep_model = Separator.from_hparams(\n",
    "        source=config.sep_model_source,\n",
    "        savedir=config.sep_model_savedir,\n",
    "        run_opts={\"device\": config.device}\n",
    "    )\n",
    "    enh_model = Separator.from_hparams(\n",
    "        source=config.enh_model_source,\n",
    "        savedir=config.enh_model_savedir,\n",
    "        run_opts={\"device\": config.device}\n",
    "    )\n",
    "\n",
    "    id_pretrained = SpeakerIdentifier(config.hubert_pretrained, config.device)\n",
    "    id_finetuned = SpeakerIdentifier(config.hubert_finetuned, config.device)\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for sample in tqdm(test_dataset, desc=\"Processing samples\"):\n",
    "        try:\n",
    "            ref_s1 = sample['s1']\n",
    "            ref_s2 = sample['s2']\n",
    "            \n",
    "            mix = sample['mix'].unsqueeze(0).to(config.device)\n",
    "            with torch.no_grad():\n",
    "                est_sources = sep_model.separate_batch(mix).permute(0, 2, 1)\n",
    "                est_sources = est_sources[0].cpu()\n",
    "                est_s1, est_s2 = est_sources[0], est_sources[1]\n",
    "\n",
    "            def enhance(wav):\n",
    "                # Resample to 16kHz for enhancement\n",
    "                wav_16k = torchaudio.functional.resample(\n",
    "                    wav, \n",
    "                    Config.sample_rate, \n",
    "                    16000\n",
    "                )\n",
    "                enhanced_16k = enh_model.separate_batch(\n",
    "                    wav_16k.unsqueeze(0).to(config.device)\n",
    "                )[0].squeeze().cpu()\n",
    "                # Resample back to original rate\n",
    "                return torchaudio.functional.resample(\n",
    "                    enhanced_16k, \n",
    "                    16000, \n",
    "                    Config.sample_rate\n",
    "                )\n",
    "\n",
    "            enh_s1 = enhance(est_s1)\n",
    "            enh_s2 = enhance(est_s2)\n",
    "\n",
    "            # Metric calculation with channel alignment\n",
    "            metrics_1 = {\n",
    "                's1': calculate_metrics(ref_s1, enh_s1),\n",
    "                's2': calculate_metrics(ref_s2, enh_s2)\n",
    "            }\n",
    "            metrics_2 = {\n",
    "                's1': calculate_metrics(ref_s1, enh_s2),\n",
    "                's2': calculate_metrics(ref_s2, enh_s1)\n",
    "            }\n",
    "\n",
    "            if (metrics_1['s1']['sdr'] + metrics_1['s2']['sdr']) > \\\n",
    "               (metrics_2['s1']['sdr'] + metrics_2['s2']['sdr']):\n",
    "                metrics = metrics_1\n",
    "                final_enh = {'s1': enh_s1, 's2': enh_s2}\n",
    "            else:\n",
    "                metrics = metrics_2\n",
    "                final_enh = {'s1': enh_s2, 's2': enh_s1}\n",
    "\n",
    "            # Speaker Identification\n",
    "            def compute_similarity(enh, ref):\n",
    "                enh_emb_p = id_pretrained.get_embedding(enh)\n",
    "                ref_emb_p = id_pretrained.get_embedding(ref)\n",
    "                enh_emb_f = id_finetuned.get_embedding(enh)\n",
    "                ref_emb_f = id_finetuned.get_embedding(ref)\n",
    "                return {\n",
    "                    'pretrained': np.dot(enh_emb_p, ref_emb_p.T).item(),\n",
    "                    'finetuned': np.dot(enh_emb_f, ref_emb_f.T).item()\n",
    "                }\n",
    "\n",
    "            s1_sim = compute_similarity(final_enh['s1'], ref_s1)\n",
    "            s2_sim = compute_similarity(final_enh['s2'], ref_s2)\n",
    "            cross_sim1 = compute_similarity(final_enh['s1'], ref_s2)\n",
    "            cross_sim2 = compute_similarity(final_enh['s2'], ref_s1)\n",
    "\n",
    "            accuracy = {\n",
    "                'pretrained': (\n",
    "                    (s1_sim['pretrained'] > cross_sim1['pretrained']) +\n",
    "                    (s2_sim['pretrained'] > cross_sim2['pretrained'])\n",
    "                ) / 2,\n",
    "                'finetuned': (\n",
    "                    (s1_sim['finetuned'] > cross_sim1['finetuned']) +\n",
    "                    (s2_sim['finetuned'] > cross_sim2['finetuned'])\n",
    "                ) / 2\n",
    "            }\n",
    "\n",
    "            results.append({\n",
    "                'metrics': metrics,\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping sample {sample['mix_id']}: {str(e)}\")\n",
    "\n",
    "    # Robust metric aggregation\n",
    "    def safe_mean(values):\n",
    "        clean_values = [v for v in values if not np.isnan(v)]\n",
    "        return np.mean(clean_values) if clean_values else np.nan\n",
    "\n",
    "    avg_metrics = {\n",
    "        'sdr': safe_mean([(r['metrics']['s1']['sdr'] + r['metrics']['s2']['sdr'])/2 for r in results]),\n",
    "        'sir': safe_mean([(r['metrics']['s1']['sir'] + r['metrics']['s2']['sir'])/2 for r in results]),\n",
    "        'sar': safe_mean([(r['metrics']['s1']['sar'] + r['metrics']['s2']['sar'])/2 for r in results]),\n",
    "        'pesq': safe_mean([(r['metrics']['s1']['pesq'] + r['metrics']['s2']['pesq'])/2 for r in results]),\n",
    "        'rank1_pretrained': safe_mean([r['accuracy']['pretrained'] for r in results]),\n",
    "        'rank1_finetuned': safe_mean([r['accuracy']['finetuned'] for r in results])\n",
    "    }\n",
    "\n",
    "    with open('results.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Metric', 'Value'])\n",
    "        for k, v in avg_metrics.items():\n",
    "            writer.writerow([k, f\"{v:.4f}\"])\n",
    "\n",
    "    print(\"\\nFinal Evaluation Results:\")\n",
    "    print(f\"SDR: {avg_metrics['sdr']:.2f} dB\")\n",
    "    print(f\"SIR: {avg_metrics['sir']:.2f} dB\")\n",
    "    print(f\"SAR: {avg_metrics['sar']:.2f} dB\")\n",
    "    print(f\"PESQ: {avg_metrics['pesq']:.2f}\")\n",
    "    print(f\"Rank-1 Accuracy (Pretrained): {avg_metrics['rank1_pretrained']:.2%}\")\n",
    "    print(f\"Rank-1 Accuracy (Finetuned): {avg_metrics['rank1_finetuned']:.2%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_sepformer()\n",
    "    evaluate_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0b4243-7175-49e8-89df-7425388c9937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
